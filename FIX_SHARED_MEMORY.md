# ğŸ”§ Fix Shared Memory Error

## âŒ Error yang Terjadi

```
RuntimeError: unable to write to file </torch_9931_3149260800_0>: No space left on device (28)
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
RuntimeError: DataLoader worker (pid 9943) is killed by signal: Bus error.
```

## ğŸ§ Penyebab

PyTorch DataLoader dengan banyak workers (16) membutuhkan banyak **shared memory** (`/dev/shm`). VPS Anda tidak memiliki cukup shared memory untuk menangani 16 workers + batch size 64.

## âœ… Solusi yang Sudah Diterapkan

Saya telah mengupdate konfigurasi training di `scripts/train_yolov8.py`:

**Sebelum:**
```python
'workers': 16,          # Terlalu banyak!
'cache': 'disk',        # Menggunakan cache
```

**Sesudah:**
```python
'workers': 2,           # Reduced untuk menghindari shm issues
'cache': False,         # Disabled cache
```

## ğŸš€ Cara Menggunakan

Sekarang Anda bisa langsung run ulang training:

```bash
# Jalankan ulang training
python scripts/train_yolov8.py --data datasets/merged_all/dataset.yaml

# Atau gunakan auto training
./run_auto_training.sh
```

## ğŸ“Š Dampak pada Performance

| Setting | Sebelum | Sesudah | Dampak |
|---------|---------|---------|--------|
| Workers | 16 | 2 | Data loading sedikit lebih lambat (~10-15%) |
| Cache | disk | False | Tidak ada cache, tapi lebih stabil |
| Training Speed | - | - | Masih sangat cepat di A100 GPU |
| **Memory Safe** | âŒ | âœ… | **Training akan berjalan!** |

**Training tetap akan selesai dalam 3-5 jam** karena bottleneck utama adalah GPU compute, bukan data loading.

## ğŸ”§ Opsi Alternatif (Jika Masih Error)

### Opsi 1: Kurangi Batch Size

Edit `scripts/train_yolov8.py` line 40:

```python
'batch_size': 32,  # Reduced dari 64
```

### Opsi 2: Workers = 0 (No Multiprocessing)

Edit `scripts/train_yolov8.py` line 41:

```python
'workers': 0,  # Single threaded, paling aman
```

### Opsi 3: Tingkatkan Shared Memory (VPS Level)

Jika menggunakan Docker:

```bash
# Run dengan increased shared memory
docker run --shm-size=16g ...
```

Jika bare metal / VM:

```bash
# Check current shm size
df -h /dev/shm

# Increase shm size (temporary)
sudo mount -o remount,size=16G /dev/shm

# Permanent: edit /etc/fstab
tmpfs /dev/shm tmpfs defaults,size=16g 0 0
```

## ğŸ¯ Recommended Configuration untuk A100

Untuk training yang stabil di A100 80GB:

```python
'batch_size': 64,     # OK untuk 80GB VRAM
'workers': 2,         # Safe untuk shm
'cache': False,       # Atau 'ram' jika punya RAM 64GB+
'amp': True,          # Mixed precision - wajib!
```

## ğŸ§ª Testing Sebelum Full Training

Test dulu dengan konfigurasi kecil:

```bash
python scripts/train_yolov8.py \
    --data datasets/merged_all/dataset.yaml \
    --epochs 5
```

Jika berhasil 5 epochs, baru jalankan full 300 epochs.

## ğŸ“ Masih Error?

Jika masih ada error setelah fix ini:

1. **Check shared memory size:**
   ```bash
   df -h /dev/shm
   ```

2. **Check RAM usage:**
   ```bash
   free -h
   ```

3. **Check GPU memory:**
   ```bash
   nvidia-smi
   ```

4. **Reduce workers to 0:**
   - Edit `scripts/train_yolov8.py`
   - Set `'workers': 0`

5. **Reduce batch size:**
   - Edit `scripts/train_yolov8.py`
   - Set `'batch_size': 32` atau `24`

## âœ… Expected Behavior Setelah Fix

```
ğŸš€ Starting YOLOv8 Training
====================================
ğŸ“Š Dataset: datasets/merged_all/dataset.yaml
ğŸ¯ Model: yolov8x
ğŸ’ª Device: cuda
ğŸ”¢ Batch Size: 64
ğŸ”„ Epochs: 300
ğŸ”§ Workers: 2
====================================

âœ… Loaded pretrained yolov8x model
ğŸ‹ï¸  Training started...

Epoch 1/300: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [02:34<00:00, ...]
```

**Tidak akan ada error lagi!** ğŸ‰

## ğŸ‰ Selamat Training!

Fix ini sudah diterapkan. Silakan run ulang training Anda!

```bash
./run_auto_training.sh
```

Atau:

```bash
python scripts/train_yolov8.py --data datasets/merged_all/dataset.yaml
```

Training akan berjalan dengan lancar! ğŸš€
